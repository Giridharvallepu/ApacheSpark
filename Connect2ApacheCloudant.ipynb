{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to insert the credentials to the Apache CouchDB / Cloudant database where your sensor data ist stored to. \n",
    "\n",
    "1. In the project's overview tab of this project just select \"Add to project\"->Connection\n",
    "2. From the section \"Your service instances in IBM Cloud\" select your cloudant database and click on \"create\"\n",
    "3. Now click in the empty cell below labeled with \"your cloudant credentials go here\"\n",
    "4. Click on the \"10-01\" symbol top right and selecrt the \"Connections\" tab\n",
    "5. Find your data base connection and click on \"Insert to code\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your cloudant credentials go here\n",
    "# @hidden_cell\n",
    "credentials_1 = {\n",
    "  'password':\"\"\"\"\"\",\n",
    "  'custom_url':'',\n",
    "  'username':'',\n",
    "  'url':'https://undefined'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Cloudant Spark SQL Example in Python using temp tables\")\\\n",
    "    .config(\"cloudant.host\",credentials_1['custom_url'].split('@')[1])\\\n",
    "    .config(\"cloudant.username\", credentials_1['username'])\\\n",
    "    .config(\"cloudant.password\",credentials_1['password'])\\\n",
    "    .config(\"jsonstore.rdd.partitions\", 1)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----+-----+-----+--------------------+--------------------+\n",
      "|CLASS| SENSORID|    X|    Y|    Z|                 _id|                _rev|\n",
      "+-----+---------+-----+-----+-----+--------------------+--------------------+\n",
      "|    0|guru1dimp| 0.06| 0.06| 0.06|0435fc55fab4b5e7e...|1-5c87cc9f82d957b...|\n",
      "|    0|guru1dimp| 0.06| 0.06| 0.06|0435fc55fab4b5e7e...|1-5c87cc9f82d957b...|\n",
      "|    0|guru1dimp| 0.06| 0.06| 0.06|0435fc55fab4b5e7e...|1-5c87cc9f82d957b...|\n",
      "|    0|guru1dimp| 0.11| 0.11| 0.11|0435fc55fab4b5e7e...|1-c987397f87e99eb...|\n",
      "|    0|guru1dimp| 0.04| 0.04| 0.04|0435fc55fab4b5e7e...|1-d89621a962c60e4...|\n",
      "|    0|guru1dimp| 0.06| 0.06| 0.06|0435fc55fab4b5e7e...|1-5c87cc9f82d957b...|\n",
      "|    0|guru1dimp| 0.01| 0.01| 0.01|0435fc55fab4b5e7e...|1-42d0a5d92988247...|\n",
      "|    0|guru1dimp| 0.03| 0.03| 0.03|0435fc55fab4b5e7e...|1-cbea04068dbe636...|\n",
      "|    0|guru1dimp| 0.01| 0.01| 0.01|0435fc55fab4b5e7e...|1-42d0a5d92988247...|\n",
      "|    0|guru1dimp| 0.03| 0.03| 0.03|0435fc55fab4b5e7e...|1-cbea04068dbe636...|\n",
      "|    0|guru1dimp| 0.02| 0.02| 0.02|0435fc55fab4b5e7e...|1-1d05ba9c8d0792e...|\n",
      "|    0|guru1dimp| 0.04| 0.04| 0.04|0435fc55fab4b5e7e...|1-d89621a962c60e4...|\n",
      "|    0|guru1dimp|-0.02|-0.02|-0.02|0435fc55fab4b5e7e...|1-47352fdbb69148f...|\n",
      "|    0|guru1dimp| 0.07| 0.07| 0.07|0435fc55fab4b5e7e...|1-c7f27207234c413...|\n",
      "|    0|guru1dimp|-0.01|-0.01|-0.01|0435fc55fab4b5e7e...|1-e5b2addc048a17a...|\n",
      "|    0|guru1dimp| 0.07| 0.07| 0.07|0435fc55fab4b5e7e...|1-c7f27207234c413...|\n",
      "|    0|guru1dimp| 0.03| 0.03| 0.03|0435fc55fab4b5e7e...|1-cbea04068dbe636...|\n",
      "|    0|guru1dimp| 0.06| 0.06| 0.06|0435fc55fab4b5e7e...|1-5c87cc9f82d957b...|\n",
      "|    0|guru1dimp| 0.09| 0.09| 0.09|0435fc55fab4b5e7e...|1-f9255657c19a10b...|\n",
      "|    0|guru1dimp|  0.0|  0.0|  0.0|0435fc55fab4b5e7e...|1-8ca0bc60548a98b...|\n",
      "+-----+---------+-----+-----+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.load('shake', \"com.cloudant.spark\")\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"SELECT * from df\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(1)\n",
    "df.write.json('a2_m1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
